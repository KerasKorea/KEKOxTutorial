## Tensorflowì™€ Kerasë¥¼ í™œìš©í•œ ê°€ìƒí™”í ê°€ê²© ì˜ˆì¸¡í•˜ê¸° ğŸ’¸
[ì›ë¬¸ ë§í¬](https://medium.com/@huangkh19951228/predicting-cryptocurrency-price-with-tensorflow-and-keras-e1674b0dc58a)
> ì´ íŠœí† ë¦¬ì–¼ì€ Tensorflowì™€ Kerasë¥¼ í™œìš©í•´ì„œ ê°€ìƒí™”í ê°€ê²©ì„ ì˜ˆì¸¡í•´ë´…ë‹ˆë‹¤.

* Keras
* CNN
* LSTM, GRU

### Introduction

ê°€ìƒí™”í, íŠ¹íˆ Bitcoinì€ ìµœê·¼ ì†Œì…œ ë¯¸ë””ì–´ì™€ ê²€ìƒ‰ ì—”ì§„ì—ì„œ ê°€ì¥ í° ì¸ê¸°ë¥¼ ì–»ê³  ìˆëŠ” ê²ƒ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì§€ëŠ¥ì ì¸ ë°œëª… ì „ëµì„ ì·¨í•œë‹¤ë©´ ê·¸ë“¤(Bitcoin)ì˜ ë†’ì€ ë³€ë™ì„±ì€ ë†’ì€ ìˆ˜ìµìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤! ì´ì œëŠ” ì „ ì„¸ê³„ ëª¨ë“  ì‚¬ëŒë“¤ì´ ê°‘ìê¸° ê°€ìƒí™”íì— ëŒ€í•´ ì´ì•¼ê¸°í•˜ê¸° ì‹œì‘í•œ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë¶ˆí–‰íˆë„, indexê°€ ë¶€ì¡±í•˜ê¸° ë•Œë¬¸ì— ê¸°ì¡´ì˜ ê¸ˆìœµìƒí’ˆê³¼ ë¹„êµí•  ë•Œ ê°€ìƒí™”íëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•˜ì£ . ì´ íŠœí† ë¦¬ì–¼ì€ ê°€ìƒí™”íì˜ ë¯¸ë˜ ì¶”ì„¸ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ Bitcoinì„ ì˜ˆë¡œ ë“¤ì–´ ë”¥ëŸ¬ë‹(Deep Learning)ìœ¼ë¡œ ì´ëŸ¬í•œ ê°€ìƒí™”íì˜ ê°€ê²©ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

<br></br>
<br></br>

### Getting Started

ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ë‹¤ìŒ í™˜ê²½ê³¼ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.

1. Python 2.7
2. Tensorflow=1.2.0
3. Keras=2.1.1
4. Pandas=0.20.3
5. Numpy=1.13.3
6. h5py=2.7.0
7. sklearn=0.19.1

<br></br>
<br></br>

### Data Collection

ì˜ˆì¸¡ ë°ì´í„°ëŠ” `Kaggle` ë˜ëŠ” `Poloniex`ì—ì„œ ìˆ˜ì§‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ê´€ì„±ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ `Poloniex`ì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°ì˜ ì—´ ì´ë¦„ì´ `Kaggle`ì˜ ì—´ ì´ë¦„ê³¼ ì¼ì¹˜í•˜ë„ë¡ ë³€ê²½ë©ë‹ˆë‹¤.

<br></br>

```python
import json
import numpy as np
import os
import pandas as pd
import urllib2

# poloniex's APIì— ì—°ê²°í•©ë‹ˆë‹¤.
url = 'https://poloniex.com/public?command=returnChartData&currencyPair=USDT_BTC&start=1356998100&end=9999999999&period=300'

# APIë¥¼ í†µí•´ ì–»ì€ jsonì„ íŒŒì‹±í•˜ê³ , pandasì˜ DataFrameìœ¼ë¡œ ë°”ê¿”ì¤ë‹ˆë‹¤.
openUrl = urllib2.urlopen(url)
r = openUrl.read()
openUrl.close()
d = json.loads(r.decode())
df = pd.DataFrame(d)

original_columns=[u'close', u'date', u'high', u'low', u'open']
new_columns = ['Close','Timestamp','High','Low','Open']
df = df.loc[:,original_columns]
df.columns = new_columns
df.to_csv('data/bitcoin2015to2017.csv',index=None)
view raw
```

<br></br>
<br></br>

### Data Preparation

ì˜ˆì¸¡ì„ ìœ„í•´ ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ íŒŒì‹±í•´ì•¼ í•©ë‹ˆë‹¤. ì´ [ë¸”ë¡œê·¸](https://nicholastsmith.wordpress.com/2017/11/13/cryptocurrency-price-prediction-using-deep-learning-in-tensorflow/)ì˜ PastSampler í´ë˜ ì´ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ë¶„í• í•˜ì—¬ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ì™€ ë¼ë²¨ ë¦¬ìŠ¤íŠ¸ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì…ë ¥ í¬ê¸°(N)ëŠ” 256ì´ê³  ì¶œë ¥ í¬ê¸°(K)ëŠ” 16ì…ë‹ˆë‹¤. Poloniexì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°ëŠ” 5ë¶„ ë‹¨ìœ„ë¡œ ì²´í¬ í‘œì‹œë©ë‹ˆë‹¤. ì´ëŠ” ì…ë ¥ì´ 1280ë¶„ ë™ì•ˆ ì§€ì†ë˜ê³  ì¶œë ¥ì´ 80ë¶„ ì´ìƒì„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

<br></br>

```Python
import numpy as np
import pandas as pd

class PastSampler:
    '''
    í•™ìŠµ ë°ì´í„°(training samples)ë¥¼ ê³¼ê±° ë°ì´í„°ë¥¼ ì´ìš©í•´ì„œ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ í˜•íƒœë¥¼ ê°–ì¶°ì¤ë‹ˆë‹¤.
    '''

    def __init__(self, N, K, sliding_window = True):
        '''
        Nê°œì˜ ê³¼ê±° ë°ì´í„°ë¥¼ ì´ìš©í•´ Kê°œì˜ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
        '''
        self.K = K
        self.N = N
        self.sliding_window = sliding_window

    def transform(self, A):
        M = self.N + self.K     #í•œê°œì˜ rowë‹¹ ë°ì´í„° ê°œìˆ˜ (sample + target)
        #indexes
        if self.sliding_window:
            I = np.arange(M) + np.arange(A.shape[0] - M + 1).reshape(-1, 1)
        else:
            if A.shape[0]%M == 0:
                I = np.arange(M)+np.arange(0,A.shape[0],M).reshape(-1,1)

            else:
                I = np.arange(M)+np.arange(0,A.shape[0] -M,M).reshape(-1,1)

        B = A[I].reshape(-1, M * A.shape[1], A.shape[2])
        ci = self.N * A.shape[1]    #í•œ ë°ì´í„°ë‹¹ feature ê°œìˆ˜
        return B[:, :ci], B[:, ci:] #í•™ìŠµ matrix, íƒ€ê²Ÿ matrix

#ë°ì´í„° íŒŒì¼ ìœ„ì¹˜(path)
dfp = 'data/bitcoin2015to2017.csv'

# ê°€ê²© ë°ì´í„° ì»¬ëŸ¼(ì—´, columns)
columns = ['Close']
df = pd.read_csv(dfp)
time_stamps = df['Timestamp']
df = df.loc[:,columns]
original_df = pd.read_csv(dfp).loc[:,columns]
```

<br></br>

PastSampler í´ë˜ìŠ¤ë¥¼ ë§Œë“  í›„ ìˆ˜ì§‘ëœ ë°ì´í„°ì— ì ìš©í–ˆìŠµë‹ˆë‹¤. ì›ë˜ ë°ì´í„°ì˜ ë²”ìœ„ëŠ” 0ì—ì„œ 10000 ì‚¬ì´ì´ë¯€ë¡œ, ì‹ ê²½ë§ì´ ë°ì´í„°ë¥¼ ë” ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë°ì´í„° ìŠ¤ì¼€ì¼ë§ì´ í•„ìš”í•©ë‹ˆë‹¤.

<br></br>

```Python
file_name='bitcoin2015to2017_close.h5'

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
# normalization
for c in columns:
    df[c] = scaler.fit_transform(df[c].values.reshape(-1,1))

#Features are input sample dimensions(channels)
A = np.array(df)[:,None,:]
original_A = np.array(original_df)[:,None,:]
time_stamps = np.array(time_stamps)[:,None,None]

#Make samples of temporal sequences of pricing data (channel)
NPS, NFS = 256, 16         #ê³¼ê±° ë°ì´í„°, ë¯¸ë˜ ë°ì´í„° ê°œìˆ˜
ps = PastSampler(NPS, NFS, sliding_window=False)
B, Y = ps.transform(A)
input_times, output_times = ps.transform(time_stamps)
original_B, original_Y = ps.transform(original_A)

import h5py
with h5py.File(file_name, 'w') as f:
    f.create_dataset("inputs", data = B)
    f.create_dataset('outputs', data = Y)
    f.create_dataset("input_times", data = input_times)
    f.create_dataset('output_times', data = output_times)
    f.create_dataset("original_datas", data=np.array(original_df))
    f.create_dataset('original_inputs',data=original_B)
    f.create_dataset('original_outputs',data=original_Y)
```

<br></br>
<br></br>

#### Building Models

##### CNN
1D CNN(Convolutional Neural Network)ì€ ì»¤ë„ì´ ì…ë ¥ë°ì´í„° ìœ„ë¥¼ ìŠ¬ë¼ì´ë”©í•˜ë©´ì„œ ì§€ì—­ì ì¸(ìœ„ì¹˜ì˜) íŠ¹ì§•ì„ ì˜ ì¡ì•„ëƒ…ë‹ˆë‹¤. figure1ì„ í•œ ë²ˆ ë³´ì„¸ìš”.

<br></br>

![CNN Illustration](./media/21_0.png)

*figure1 : CNN Illustration (retrieved from http://cs231n.github.io/convolutional-networks/)*

<br></br>

```Python
import pandas as pd
import numpy as numpy
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv1D, MaxPooling1D, LeakyReLU, PReLU
from keras.utils import np_utils
from keras.callbacks import CSVLogger, ModelCheckpoint
import h5py
import os
import tensorflow as tf
from keras.backend.tensorflow_backend import set_session


# í•œ ê°œì˜ GPUë§Œì„ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

config = tf.ConfigProto()
config.gpu_options.allow_growth = True
set_session(tf.Session(config=config))


with h5py.File(''.join(['bitcoin2015to2017_close.h5']), 'r') as hf:
    datas = hf['inputs'].value
    labels = hf['outputs'].value


output_file_name='bitcoin2015to2017_close_CNN_2_relu'

step_size = datas.shape[1]
batch_size= 8
nb_features = datas.shape[2]

epochs = 100

# ë°ì´í„°ë¥¼ train, validation ìœ¼ë¡œ ë‚˜ëˆ”
training_size = int(0.8* datas.shape[0])
training_datas = datas[:training_size,:]
training_labels = labels[:training_size,:]
validation_datas = datas[training_size:,:]
validation_labels = labels[training_size:,:]
#build model

# 2 layers
model = Sequential()


model.add(Conv1D(activation='relu', input_shape=(step_size, nb_features), strides=3, filters=8, kernel_size=20))
model.add(Dropout(0.5))
model.add(Conv1D( strides=4, filters=nb_features, kernel_size=16))

'''
# 3 Layers
model.add(Conv1D(activation='relu', input_shape=(step_size, nb_features), strides=3, filters=8, kernel_size=8))
#model.add(LeakyReLU())
model.add(Dropout(0.5))
model.add(Conv1D(activation='relu', strides=2, filters=8, kernel_size=8))
#model.add(LeakyReLU())
model.add(Dropout(0.5))
model.add(Conv1D( strides=2, filters=nb_features, kernel_size=8))
# 4 layers
model.add(Conv1D(activation='relu', input_shape=(step_size, nb_features), strides=2, filters=8, kernel_size=2))
#model.add(LeakyReLU())
model.add(Dropout(0.5))
model.add(Conv1D(activation='relu', strides=2, filters=8, kernel_size=2))
#model.add(LeakyReLU())
model.add(Dropout(0.5))
model.add(Conv1D(activation='relu', strides=2, filters=8, kernel_size=2))
#model.add(LeakyReLU())
model.add(Dropout(0.5))
model.add(Conv1D( strides=2, filters=nb_features, kernel_size=2))
'''
model.compile(loss='mse', optimizer='adam')
model.fit(training_datas, training_labels,verbose=1, batch_size=batch_size,validation_data=(validation_datas,validation_labels), epochs = epochs, callbacks=[CSVLogger(output_file_name+'.csv', append=True),ModelCheckpoint('weights/'+output_file_name+'-{epoch:02d}-{val_loss:.5f}.hdf5', monitor='val_loss', verbose=1,mode='min')])
```

<br></br>

ë‚´ê°€ ë¹Œë“œí•œ ì²« ë²ˆì§¸ ëª¨ë¸ì€ CNN ì…ë‹ˆë‹¤. ìœ„ì˜ ì½”ë“œì—ì„œëŠ” ì‚¬ìš©í•  GPU ê°œìˆ˜ë¥¼ "1"ë¡œ ì„¤ì •í•©ë‹ˆë‹¤(ë‚˜ëŠ” 4ê°œì˜ GPUë¥¼ ê°€ì§€ê³  ìˆì§€ë§Œ, ë‹¹ì‹ ì€ ì›í•˜ëŠ” ë§Œí¼ì˜ GPUë¥¼ ìƒìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤). ì™œëƒí•˜ë©´ Tensorflowë¡œ ì—¬ëŸ¬ê°œì˜ GPUsë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ê·¸ë‹¤ì§€ ì˜ ëŒì•„ê°€ì§€ ì•Šê¸° ë•Œë¬¸ì—, ì‚¬ìš©í•  GPUë¥¼ 1ê°œë¡œ ì œí•œí•˜ëŠ” ê²ƒì´ ë” í˜„ëª…í•œ ë°©ë²•ì¼ì§€ë„ ëª¨ë¦…ë‹ˆë‹¤. ë‹¹ì‹ ì´ GPUê°€ ì—†ë‹¤ê³ í•´ë„ ê±±ì •í•˜ì§€ë§ˆì„¸ìš”. GPUë¥¼ ì„¤ì •í•˜ëŠ” ì½”ë“œë¥¼ ê°€ë¿íˆ ë¬´ì‹œí•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.

```Python
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
os.environ['CUDA_VISIBLE_DEVICES'] ='1'
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'
```

<br></br>

CNN ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•œ ì½”ë“œëŠ” ì•„ì£¼ ê°„ë‹¨í•©ë‹ˆë‹¤. dropout ë ˆì´ì–´ëŠ” overfitting(ê³¼ì í•©)ì„ ë§‰ì•„ì¤ë‹ˆë‹¤. loss function(ì†ì‹¤í•¨ìˆ˜)ëŠ” Mean Squared Error(MSE)ë¡œ ì •ì˜í–ˆìŠµë‹ˆë‹¤. optimizerëŠ” Adamì„ ì‚¬ìš©í• ê±°ì—ìš”.

```Python
model = Sequential()
model.add(Conv1D(activation='relu', input_shape=(step_size, nb_features), strides=3, filters=8, kernel_size=20))
model.add(Dropout(0.5))
model.add(Conv1D( strides=4, filters=nb_features, kernel_size=16))
model.compile(loss='mse', optimizer='adam')
```

<br></br>

ë‹¹ì‹ ì´ ê±±ì •í•´ì•¼í•  ë‹¨ í•œê°€ì§€ëŠ” ê° ë ˆì´ì–´ì˜ ì…ë ¥, ì¶œë ¥ ì°¨ì›(dimension)ì…ë‹ˆë‹¤. íŠ¹ì • convolutional ê³„ì¸µì˜ ì¶œë ¥ì„ ê³„ì‚°í•˜ëŠ” ê³µì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

> **Output time step = (Input time stepâ€Šâ€”â€ŠKernel size) / Strides + 1**

<br></br>

íŒŒì¼ì˜ ë§ˆì§€ë§‰ì—, ë‚˜ëŠ” ë‘ê°œì˜ callback í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•  ê²ƒì…ë‹ˆë‹¤. í•œ ê°œëŠ” CSVLogger ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ModelCheckpoint ì…ë‹ˆë‹¤. ì²« ë²ˆì§¸ CSVLogger í•¨ìˆ˜ëŠ” training, validation ì„ íŠ¸ë™í‚¹í•˜ëŠ” ê²ƒì„ ë„ì™€ì¤„ ê²ƒì…ë‹ˆë‹¤. ë‘ ë²ˆì§¸ ModelCheckpoint í•¨ìˆ˜ëŠ” ë§¤ epoch ë§ˆë‹¤ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜(weight)ë¥¼ ì €ì¥í•´ ì¤ë‹ˆë‹¤.

```Python
model.fit(training_datas, training_labels,verbose=1, batch_size=batch_size,validation_data=(validation_datas,validation_labels), epochs = epochs, callbacks=[CSVLogger(output_file_name+'.csv', append=True),ModelCheckpoint('weights/'+output_file_name+'-{epoch:02d}-{val_loss:.5f}.hdf5', monitor='val_loss', verbose=1,mode='min')]
```

<br></br>
<br></br>

#### LSTM

Long Short Term Memory(LSTM) ë„¤íŠ¸ì›Œí¬ëŠ” Recurrent Neural Network(RNN)ì˜ ì¼ì¢…ì…ë‹ˆë‹¤. vanilla RNNì˜ `vanishing gradient problem`ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤. LSTMì€ ë” ê¸´ ì‹œê°„ ë™ì•ˆ ì…ë ¥ì„ ê¸°ì–µí•  ìˆ˜ ìˆë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤.

<br></br>

> RNNì˜ `vanishing gradient problem`ì€ ê´€ë ¨ëœ ì •ë³´ì™€ ê·¸ ì •ë³´ë¥¼ ì°¸ì¡°í•´ì•¼ë˜ëŠ” ì§€ì (ì •ë³´ë¥¼ ì‚¬ìš©í•˜ëŠ” ì§€ì )ì˜ ì‚¬ì´ê°€ ë©€ì–´ì„œ Backpropagation(ì—­ì „íŒŒ)ë¥¼ í•  ë•Œ, gradientê°€ ì¤„ì–´ë“¤ì–´ í•™ìŠµ ëŠ¥ë ¥ì´ ì €í•˜ë˜ëŠ” ë¬¸ì œì…ë‹ˆë‹¤. LSTMì€ ê¸°ì¡´ì˜ vanilla RNNì— cell stateë¥¼ ì¶”ê°€í•˜ì—¬ ê³¼ê±°ì˜ ì •ë³´ê°€ ì–¼ë§ˆë§Œí¼ ìœ ì§€ë  ê²ƒì¸ì§€ ë“±ì„ ì¡°ì ˆí•´ì„œ vanishing gradeint problemì„ í•´ê²°í•©ë‹ˆë‹¤.

<br></br>

![LSTM](./media/21_1.png)

*figure2 : LSTM Illustration (retrieved from http://colah.github.io/posts/2015-08-Understanding-LSTMs/)*

<br></br>

```Python
import pandas as pd
import numpy as numpy
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten,Reshape
from keras.layers import Conv1D, MaxPooling1D
from keras.utils import np_utils
from keras.layers import LSTM, LeakyReLU
from keras.callbacks import CSVLogger, ModelCheckpoint
import h5py
import os
import tensorflow as tf
from keras.backend.tensorflow_backend import set_session



os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

config = tf.ConfigProto()
config.gpu_options.allow_growth = True
set_session(tf.Session(config=config))

with h5py.File(''.join(['bitcoin2015to2017_close.h5']), 'r') as hf:
    datas = hf['inputs'].value
    labels = hf['outputs'].value




step_size = datas.shape[1]
units= 50
second_units = 30
batch_size = 8
nb_features = datas.shape[2]
epochs = 100
output_size=16
output_file_name='bitcoin2015to2017_close_LSTM_1_tanh_leaky_'
# ë°ì´í„°ë¥¼ train, validation ìœ¼ë¡œ ë‚˜ëˆ”
training_size = int(0.8* datas.shape[0])
training_datas = datas[:training_size,:]
training_labels = labels[:training_size,:,0]
validation_datas = datas[training_size:,:]
validation_labels = labels[training_size:,:,0]


#build model
model = Sequential()
model.add(LSTM(units=units,activation='tanh', input_shape=(step_size,nb_features),return_sequences=False))
model.add(Dropout(0.8))
model.add(Dense(output_size))
model.add(LeakyReLU())
model.compile(loss='mse', optimizer='adam')
model.fit(training_datas, training_labels, batch_size=batch_size,validation_data=(validation_datas,validation_labels), epochs = epochs, callbacks=[CSVLogger(output_file_name+'.csv', append=True),ModelCheckpoint('weights/'+output_file_name+'-{epoch:02d}-{val_loss:.5f}.hdf5', monitor='val_loss', verbose=1,mode='min')])
```

<br></br>

LSTMì€ ì»¤ë„ í¬ê¸°, strides, ì…ë ¥ ì‚¬ì´ì¦ˆ ë° ì¶œë ¥ ì‚¬ì´ì¦ˆ ê°„ì˜ ê´€ê³„ë¥¼ ì‹ ê²½ ì“¸ í•„ìš”ê°€ ì—†ìœ¼ë¯€ë¡œ êµ¬í˜„ì´ CNNë³´ë‹¤ ìƒëŒ€ì ìœ¼ë¡œ ì‰½ìŠµë‹ˆë‹¤. ì…ë ¥ ë° ì¶œë ¥ì˜ ì‚¬ì´ì¦ˆê°€ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì˜¬ë°”ë¥´ê²Œ ì •ì˜ë˜ì—ˆëŠ”ì§€ë§Œ í™•ì¸í•˜ì„¸ìš”!

```Python
model = Sequential()
model.add(LSTM(units=units,activation='tanh', input_shape=(step_size,nb_features),return_sequences=False))
model.add(Dropout(0.8))
model.add(Dense(output_size))
model.add(LeakyReLU())
model.compile(loss='mse', optimizer='adam')
```

<br></br>
<br></br>

#### GRU

Gated Recurrent Unit(Gated Recurrent Unit)ì€ RNNì˜ ë˜ ë‹¤ë¥¸ ë³€í˜•ì…ë‹ˆë‹¤. GRUì˜ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ëŠ” 1ê°œì˜ reset, forget ê²Œì´íŠ¸ê°€ ìˆì–´ì„œ LSTMë³´ë‹¤ ëœ ì •êµí•˜ì§€ë§Œ GRUì˜ ì„±ëŠ¥ì€ LSTMê³¼ ë™ì¼í•©ë‹ˆë‹¤. ë”°ë¼ì„œ LSTM ë³´ë‹¤ íš¨ìœ¨ì„±ì´ ë” ë†’ë‹¤ëŠ” ì£¼ì¥ì´ ìˆìŠµë‹ˆë‹¤. (ì´ íŠœí† ë¦¬ì–¼ì—ì„œë„ LSTMëŠ” ì•½ 45ì´ˆ/epoch, GRUëŠ” 40ì´ˆ/epoch ì±„ ê±¸ë¦¬ì§€ ì•Šê¸° ë•Œë¬¸ì— ì£¼ì¥ì´ ë§ë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)

<br></br>

![GRU](./media/21_2.png)

*GRU Illustration (retrieved from http://www.jackdermody.net/brightwire/article/GRU_Recurrent_Neural_Networks)*

<br></br>

```Python
import pandas as pd
import numpy as numpy
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten,Reshape
from keras.layers import Conv1D, MaxPooling1D, LeakyReLU
from keras.utils import np_utils
from keras.layers import GRU,CuDNNGRU
from keras.callbacks import CSVLogger, ModelCheckpoint
import h5py
import os
import tensorflow as tf
from keras.backend.tensorflow_backend import set_session



os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

config = tf.ConfigProto()
config.gpu_options.allow_growth = True
set_session(tf.Session(config=config))

with h5py.File(''.join(['bitcoin2015to2017_close.h5']), 'r') as hf:
    datas = hf['inputs'].value
    labels = hf['outputs'].value


output_file_name='bitcoin2015to2017_close_GRU_1_tanh_relu_'

step_size = datas.shape[1]
units= 50
batch_size = 8
nb_features = datas.shape[2]
epochs = 100
output_size=16
#split training validation
training_size = int(0.8* datas.shape[0])
training_datas = datas[:training_size,:]
training_labels = labels[:training_size,:,0]
validation_datas = datas[training_size:,:]
validation_labels = labels[training_size:,:,0]

#build model
model = Sequential()
model.add(GRU(units=units, input_shape=(step_size,nb_features),return_sequences=False))
model.add(Activation('tanh'))
model.add(Dropout(0.2))
model.add(Dense(output_size))
model.add(Activation('relu'))
model.compile(loss='mse', optimizer='adam')
model.fit(training_datas, training_labels, batch_size=batch_size,validation_data=(validation_datas,validation_labels), epochs = epochs, callbacks=[CSVLogger(output_file_name+'.csv', append=True),ModelCheckpoint('weights/'+output_file_name+'-{epoch:02d}-{val_loss:.5f}.hdf5', monitor='val_loss', verbose=1,mode='min')])
```

<br></br>

ê°„ë‹¨í•˜ê²Œ LSTMì„ ë¹Œë“œí•˜ëŠ” ë‘ ë²ˆì§¸ ì½”ë“œë¼ì¸ì„ GRUë¡œ ë°”ê¾¸ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.

```Python
model.add(LSTM(units=units,activation='tanh', input_shape=(step_size,nb_features),return_sequences=False))
```

ì„

```Python
model.add(GRU(units=units,activation='tanh', input_shape=(step_size,nb_features),return_sequences=False))
```

ë¡œ ë°”ê¾¸ì„¸ìš”!

<br></br>
<br></br>



#### ì°¸ê³ ìë£Œ
[RNNê³¼ LSTMì„ ì´í•´í•´ë³´ì!](https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/)
