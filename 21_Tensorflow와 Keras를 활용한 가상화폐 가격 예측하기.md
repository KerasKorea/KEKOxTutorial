## Tensorflowì™€ Kerasë¥¼ í™œìš©í•œ ê°€ìƒí™”í ê°€ê²© ì˜ˆì¸¡í•˜ê¸° ğŸ’¸
[ì›ë¬¸ ë§í¬](https://medium.com/@huangkh19951228/predicting-cryptocurrency-price-with-tensorflow-and-keras-e1674b0dc58a)
> ì´ íŠœí† ë¦¬ì–¼ì€ Tensorflowì™€ Kerasë¥¼ í™œìš©í•´ì„œ ê°€ìƒí™”í ê°€ê²©ì„ ì˜ˆì¸¡í•´ë´…ë‹ˆë‹¤.

* Keras
* CNN
* LSTM, GRU

### Introduction

ê°€ìƒí™”í, íŠ¹íˆ Bitcoinì€ ìµœê·¼ ì†Œì…œ ë¯¸ë””ì–´ì™€ ê²€ìƒ‰ ì—”ì§„ì—ì„œ ê°€ì¥ í° ì¸ê¸°ë¥¼ ì–»ê³  ìˆëŠ” ê²ƒ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì§€ëŠ¥ì ì¸ ë°œëª… ì „ëµì„ ì·¨í•œë‹¤ë©´ ê·¸ë“¤(Bitcoin)ì˜ ë†’ì€ ë³€ë™ì„±ì€ ë†’ì€ ìˆ˜ìµìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤! ì´ì œëŠ” ì „ ì„¸ê³„ ëª¨ë“  ì‚¬ëŒë“¤ì´ ê°‘ìê¸° ê°€ìƒí™”íì— ëŒ€í•´ ì´ì•¼ê¸°í•˜ê¸° ì‹œì‘í•œ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë¶ˆí–‰íˆë„, indexê°€ ë¶€ì¡±í•˜ê¸° ë•Œë¬¸ì— ê¸°ì¡´ì˜ ê¸ˆìœµìƒí’ˆê³¼ ë¹„êµí•  ë•Œ ê°€ìƒí™”íëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•˜ì£ . ì´ íŠœí† ë¦¬ì–¼ì€ ê°€ìƒí™”íì˜ ë¯¸ë˜ ì¶”ì„¸ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ Bitcoinì„ ì˜ˆë¡œ ë“¤ì–´ ë”¥ëŸ¬ë‹(Deep Learning)ìœ¼ë¡œ ì´ëŸ¬í•œ ê°€ìƒí™”íì˜ ê°€ê²©ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

<br></br>
<br></br>

### Getting Started

ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ë‹¤ìŒ í™˜ê²½ê³¼ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.

1. Python 2.7
2. Tensorflow=1.2.0
3. Keras=2.1.1
4. Pandas=0.20.3
5. Numpy=1.13.3
6. h5py=2.7.0
7. sklearn=0.19.1

<br></br>
<br></br>

### Data Collection

ì˜ˆì¸¡ ë°ì´í„°ëŠ” `Kaggle` ë˜ëŠ” `Poloniex`ì—ì„œ ìˆ˜ì§‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ê´€ì„±ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ `Poloniex`ì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°ì˜ ì—´ ì´ë¦„ì´ `Kaggle`ì˜ ì—´ ì´ë¦„ê³¼ ì¼ì¹˜í•˜ë„ë¡ ë³€ê²½ë©ë‹ˆë‹¤.

<br></br>

```python
import json
import numpy as np
import os
import pandas as pd
import urllib2

# poloniex's APIì— ì—°ê²°í•©ë‹ˆë‹¤.
url = 'https://poloniex.com/public?command=returnChartData&currencyPair=USDT_BTC&start=1356998100&end=9999999999&period=300'

# APIë¥¼ í†µí•´ ì–»ì€ jsonì„ íŒŒì‹±í•˜ê³ , pandasì˜ DataFrameìœ¼ë¡œ ë°”ê¿”ì¤ë‹ˆë‹¤.
openUrl = urllib2.urlopen(url)
r = openUrl.read()
openUrl.close()
d = json.loads(r.decode())
df = pd.DataFrame(d)

original_columns=[u'close', u'date', u'high', u'low', u'open']
new_columns = ['Close','Timestamp','High','Low','Open']
df = df.loc[:,original_columns]
df.columns = new_columns
df.to_csv('data/bitcoin2015to2017.csv',index=None)
view raw
```

<br></br>
<br></br>

### Data Preparation

ì˜ˆì¸¡ì„ ìœ„í•´ ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ íŒŒì‹±í•´ì•¼ í•©ë‹ˆë‹¤. ì´ [ë¸”ë¡œê·¸](https://nicholastsmith.wordpress.com/2017/11/13/cryptocurrency-price-prediction-using-deep-learning-in-tensorflow/)ì˜ PastSampler í´ë˜ ì´ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ë¶„í• í•˜ì—¬ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ì™€ ë¼ë²¨ ë¦¬ìŠ¤íŠ¸ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì…ë ¥ í¬ê¸°(N)ëŠ” 256ì´ê³  ì¶œë ¥ í¬ê¸°(K)ëŠ” 16ì…ë‹ˆë‹¤. Poloniexì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°ëŠ” 5ë¶„ ë‹¨ìœ„ë¡œ ì²´í¬ í‘œì‹œë©ë‹ˆë‹¤. ì´ëŠ” ì…ë ¥ì´ 1280ë¶„ ë™ì•ˆ ì§€ì†ë˜ê³  ì¶œë ¥ì´ 80ë¶„ ì´ìƒì„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

<br></br>

```Python
import numpy as np
import pandas as pd

class PastSampler:
    '''
    í•™ìŠµ ë°ì´í„°(training samples)ë¥¼ ê³¼ê±° ë°ì´í„°ë¥¼ ì´ìš©í•´ì„œ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ í˜•íƒœë¥¼ ê°–ì¶°ì¤ë‹ˆë‹¤.
    '''

    def __init__(self, N, K, sliding_window = True):
        '''
        Nê°œì˜ ê³¼ê±° ë°ì´í„°ë¥¼ ì´ìš©í•´ Kê°œì˜ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
        '''
        self.K = K
        self.N = N
        self.sliding_window = sliding_window

    def transform(self, A):
        M = self.N + self.K     #í•œê°œì˜ rowë‹¹ ë°ì´í„° ê°œìˆ˜ (sample + target)
        #indexes
        if self.sliding_window:
            I = np.arange(M) + np.arange(A.shape[0] - M + 1).reshape(-1, 1)
        else:
            if A.shape[0]%M == 0:
                I = np.arange(M)+np.arange(0,A.shape[0],M).reshape(-1,1)

            else:
                I = np.arange(M)+np.arange(0,A.shape[0] -M,M).reshape(-1,1)

        B = A[I].reshape(-1, M * A.shape[1], A.shape[2])
        ci = self.N * A.shape[1]    #í•œ ë°ì´í„°ë‹¹ feature ê°œìˆ˜
        return B[:, :ci], B[:, ci:] #í•™ìŠµ matrix, íƒ€ê²Ÿ matrix

#ë°ì´í„° íŒŒì¼ ìœ„ì¹˜(path)
dfp = 'data/bitcoin2015to2017.csv'

# ê°€ê²© ë°ì´í„° ì»¬ëŸ¼(ì—´, columns)
columns = ['Close']
df = pd.read_csv(dfp)
time_stamps = df['Timestamp']
df = df.loc[:,columns]
original_df = pd.read_csv(dfp).loc[:,columns]
```

<br></br>

PastSampler í´ë˜ìŠ¤ë¥¼ ë§Œë“  í›„ ìˆ˜ì§‘ëœ ë°ì´í„°ì— ì ìš©í–ˆìŠµë‹ˆë‹¤. ì›ë˜ ë°ì´í„°ì˜ ë²”ìœ„ëŠ” 0ì—ì„œ 10000 ì‚¬ì´ì´ë¯€ë¡œ, ì‹ ê²½ë§ì´ ë°ì´í„°ë¥¼ ë” ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë°ì´í„° ìŠ¤ì¼€ì¼ë§ì´ í•„ìš”í•©ë‹ˆë‹¤.

<br></br>

```Python
file_name='bitcoin2015to2017_close.h5'

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
# normalization
for c in columns:
    df[c] = scaler.fit_transform(df[c].values.reshape(-1,1))

#Features are input sample dimensions(channels)
A = np.array(df)[:,None,:]
original_A = np.array(original_df)[:,None,:]
time_stamps = np.array(time_stamps)[:,None,None]

#Make samples of temporal sequences of pricing data (channel)
NPS, NFS = 256, 16         #ê³¼ê±° ë°ì´í„°, ë¯¸ë˜ ë°ì´í„° ê°œìˆ˜
ps = PastSampler(NPS, NFS, sliding_window=False)
B, Y = ps.transform(A)
input_times, output_times = ps.transform(time_stamps)
original_B, original_Y = ps.transform(original_A)

import h5py
with h5py.File(file_name, 'w') as f:
    f.create_dataset("inputs", data = B)
    f.create_dataset('outputs', data = Y)
    f.create_dataset("input_times", data = input_times)
    f.create_dataset('output_times', data = output_times)
    f.create_dataset("original_datas", data=np.array(original_df))
    f.create_dataset('original_inputs',data=original_B)
    f.create_dataset('original_outputs',data=original_Y)
```

<br></br>
<br></br>

#### Building Models

##### CNN
1D CNN(Convolutional Neural Network)ì€ ì»¤ë„ì´ ì…ë ¥ë°ì´í„° ìœ„ë¥¼ ìŠ¬ë¼ì´ë”©í•˜ë©´ì„œ ì§€ì—­ì ì¸(ìœ„ì¹˜ì˜) íŠ¹ì§•ì„ ì˜ ì¡ì•„ëƒ…ë‹ˆë‹¤. figure1ì„ í•œ ë²ˆ ë³´ì„¸ìš”.

<br></br>

![CNN Illustration](./media/21_0.png)

*figure1 : CNN Illustration (retrieved from http://cs231n.github.io/convolutional-networks/)*

<br></br>

```Python
import pandas as pd
import numpy as numpy
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv1D, MaxPooling1D, LeakyReLU, PReLU
from keras.utils import np_utils
from keras.callbacks import CSVLogger, ModelCheckpoint
import h5py
import os
import tensorflow as tf
from keras.backend.tensorflow_backend import set_session


# í•œ ê°œì˜ GPUë§Œì„ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

config = tf.ConfigProto()
config.gpu_options.allow_growth = True
set_session(tf.Session(config=config))


with h5py.File(''.join(['bitcoin2015to2017_close.h5']), 'r') as hf:
    datas = hf['inputs'].value
    labels = hf['outputs'].value


output_file_name='bitcoin2015to2017_close_CNN_2_relu'

step_size = datas.shape[1]
batch_size= 8
nb_features = datas.shape[2]

epochs = 100

# ë°ì´í„°ë¥¼ train, validation ìœ¼ë¡œ ë‚˜ëˆ”
training_size = int(0.8* datas.shape[0])
training_datas = datas[:training_size,:]
training_labels = labels[:training_size,:]
validation_datas = datas[training_size:,:]
validation_labels = labels[training_size:,:]
#build model

# 2 layers
model = Sequential()


model.add(Conv1D(activation='relu', input_shape=(step_size, nb_features), strides=3, filters=8, kernel_size=20))
model.add(Dropout(0.5))
model.add(Conv1D( strides=4, filters=nb_features, kernel_size=16))

'''
# 3 Layers
model.add(Conv1D(activation='relu', input_shape=(step_size, nb_features), strides=3, filters=8, kernel_size=8))
#model.add(LeakyReLU())
model.add(Dropout(0.5))
model.add(Conv1D(activation='relu', strides=2, filters=8, kernel_size=8))
#model.add(LeakyReLU())
model.add(Dropout(0.5))
model.add(Conv1D( strides=2, filters=nb_features, kernel_size=8))
# 4 layers
model.add(Conv1D(activation='relu', input_shape=(step_size, nb_features), strides=2, filters=8, kernel_size=2))
#model.add(LeakyReLU())
model.add(Dropout(0.5))
model.add(Conv1D(activation='relu', strides=2, filters=8, kernel_size=2))
#model.add(LeakyReLU())
model.add(Dropout(0.5))
model.add(Conv1D(activation='relu', strides=2, filters=8, kernel_size=2))
#model.add(LeakyReLU())
model.add(Dropout(0.5))
model.add(Conv1D( strides=2, filters=nb_features, kernel_size=2))
'''
model.compile(loss='mse', optimizer='adam')
model.fit(training_datas, training_labels,verbose=1, batch_size=batch_size,validation_data=(validation_datas,validation_labels), epochs = epochs, callbacks=[CSVLogger(output_file_name+'.csv', append=True),ModelCheckpoint('weights/'+output_file_name+'-{epoch:02d}-{val_loss:.5f}.hdf5', monitor='val_loss', verbose=1,mode='min')])
```

<br></br>

ì—¬ê¸°ì„œ ë¹Œë“œí•œ ì²« ë²ˆì§¸ ëª¨ë¸ì€ CNNë¡œ, ì‚¬ìš©í•  GPU ê°œìˆ˜ë¥¼ "1"ë¡œ ì„¤ì •í•©ë‹ˆë‹¤(ì €ëŠ” 4ê°œì˜ GPUë¥¼ ê°€ì§€ê³  ìˆì§€ë§Œ, ë‹¹ì‹ ì€ ì›í•˜ëŠ” ë§Œí¼ì˜ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤). ì™œëƒí•˜ë©´ Tensorflowë¡œ ì—¬ëŸ¬ê°œì˜ GPUsë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ê·¸ë‹¤ì§€ ì˜ ëŒì•„ê°€ì§€ ì•Šê¸° ë•Œë¬¸ì—, ì‚¬ìš©í•  GPUë¥¼ 1ê°œë¡œ ì œí•œí•˜ëŠ” ê²ƒì´ ë” í˜„ëª…í•œ ë°©ë²•ì¼ì§€ë„ ëª¨ë¦…ë‹ˆë‹¤. ë‹¹ì‹ ì´ GPUê°€ ì—†ë‹¤ê³ í•´ë„ ê±±ì •í•˜ì§€ë§ˆì„¸ìš”. GPUë¥¼ ì„¤ì •í•˜ëŠ” ì½”ë“œë¥¼ ê°€ë¿íˆ ë¬´ì‹œí•˜ë©´ ë©ë‹ˆë‹¤.

```Python
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
os.environ['CUDA_VISIBLE_DEVICES'] ='1'
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'
```

<br></br>

CNN ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•œ ì½”ë“œëŠ” ì•„ì£¼ ê°„ë‹¨í•©ë‹ˆë‹¤. dropout ë ˆì´ì–´ëŠ” overfitting(ê³¼ì í•©)ì„ ë§‰ì•„ì¤ë‹ˆë‹¤. loss function(ì†ì‹¤í•¨ìˆ˜)ëŠ” Mean Squared Error(MSE)ë¡œ ì •ì˜í–ˆìŠµë‹ˆë‹¤. optimizerëŠ” Adamì„ ì‚¬ìš©í• ê±°ì—ìš”.

```Python
model = Sequential()
model.add(Conv1D(activation='relu', input_shape=(step_size, nb_features), strides=3, filters=8, kernel_size=20))
model.add(Dropout(0.5))
model.add(Conv1D( strides=4, filters=nb_features, kernel_size=16))
model.compile(loss='mse', optimizer='adam')
```

<br></br>

ë‹¹ì‹ ì´ ê±±ì •í•´ì•¼í•  ë‹¨ í•œê°€ì§€ëŠ” ê° ë ˆì´ì–´ì˜ ì…ë ¥, ì¶œë ¥ ì°¨ì›(dimension)ì…ë‹ˆë‹¤. íŠ¹ì • convolutional ê³„ì¸µì˜ ì¶œë ¥ì„ ê³„ì‚°í•˜ëŠ” ê³µì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

> **Output time step = (Input time stepâ€Šâ€”â€ŠKernel size) / Strides + 1**

<br></br>

íŒŒì¼ì˜ ë§ˆì§€ë§‰ì—, ì €ëŠ” ë‘ê°œì˜ callback í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•  ê²ƒì…ë‹ˆë‹¤. í•œ ê°œëŠ” CSVLogger ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ModelCheckpoint ì…ë‹ˆë‹¤. ì²« ë²ˆì§¸ CSVLogger í•¨ìˆ˜ëŠ” training, validation ì„ íŠ¸ë™í‚¹í•˜ëŠ” ê²ƒì„ ë„ì™€ì¤„ ê²ƒì…ë‹ˆë‹¤. ë‘ ë²ˆì§¸ ModelCheckpoint í•¨ìˆ˜ëŠ” ë§¤ epoch ë§ˆë‹¤ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜(weight)ë¥¼ ì €ì¥í•´ ì¤ë‹ˆë‹¤.

```Python
model.fit(training_datas, training_labels,verbose=1, batch_size=batch_size,validation_data=(validation_datas,validation_labels), epochs = epochs, callbacks=[CSVLogger(output_file_name+'.csv', append=True),ModelCheckpoint('weights/'+output_file_name+'-{epoch:02d}-{val_loss:.5f}.hdf5', monitor='val_loss', verbose=1,mode='min')]
```

<br></br>
<br></br>

#### LSTM

Long Short Term Memory(LSTM) ë„¤íŠ¸ì›Œí¬ëŠ” Recurrent Neural Network(RNN)ì˜ ì¼ì¢…ì…ë‹ˆë‹¤. vanilla RNNì˜ `vanishing gradient problem`ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤. LSTMì€ ë” ê¸´ ì‹œê°„ ë™ì•ˆ ì…ë ¥ì„ ê¸°ì–µí•  ìˆ˜ ìˆë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤.

<br></br>

> RNNì˜ `vanishing gradient problem`ì€ ê´€ë ¨ëœ ì •ë³´ì™€ ê·¸ ì •ë³´ë¥¼ ì°¸ì¡°í•´ì•¼ë˜ëŠ” ì§€ì (ì •ë³´ë¥¼ ì‚¬ìš©í•˜ëŠ” ì§€ì )ì˜ ì‚¬ì´ê°€ ë©€ì–´ì„œ Backpropagation(ì—­ì „íŒŒ)ë¥¼ í•  ë•Œ, gradientê°€ ì¤„ì–´ë“¤ì–´ í•™ìŠµ ëŠ¥ë ¥ì´ ì €í•˜ë˜ëŠ” ë¬¸ì œì…ë‹ˆë‹¤. LSTMì€ ê¸°ì¡´ì˜ vanilla RNNì— cell stateë¥¼ ì¶”ê°€í•˜ì—¬ ê³¼ê±°ì˜ ì •ë³´ê°€ ì–¼ë§ˆë§Œí¼ ìœ ì§€ë  ê²ƒì¸ì§€ ë“±ì„ ì¡°ì ˆí•´ì„œ vanishing gradeint problemì„ í•´ê²°í•©ë‹ˆë‹¤.

<br></br>

![LSTM](./media/21_1.png)

*figure2 : LSTM Illustration (retrieved from http://colah.github.io/posts/2015-08-Understanding-LSTMs/)*

<br></br>

```Python
import pandas as pd
import numpy as numpy
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten,Reshape
from keras.layers import Conv1D, MaxPooling1D
from keras.utils import np_utils
from keras.layers import LSTM, LeakyReLU
from keras.callbacks import CSVLogger, ModelCheckpoint
import h5py
import os
import tensorflow as tf
from keras.backend.tensorflow_backend import set_session



os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

config = tf.ConfigProto()
config.gpu_options.allow_growth = True
set_session(tf.Session(config=config))

with h5py.File(''.join(['bitcoin2015to2017_close.h5']), 'r') as hf:
    datas = hf['inputs'].value
    labels = hf['outputs'].value




step_size = datas.shape[1]
units= 50
second_units = 30
batch_size = 8
nb_features = datas.shape[2]
epochs = 100
output_size=16
output_file_name='bitcoin2015to2017_close_LSTM_1_tanh_leaky_'
# ë°ì´í„°ë¥¼ train, validation ìœ¼ë¡œ ë‚˜ëˆ”
training_size = int(0.8* datas.shape[0])
training_datas = datas[:training_size,:]
training_labels = labels[:training_size,:,0]
validation_datas = datas[training_size:,:]
validation_labels = labels[training_size:,:,0]


#build model
model = Sequential()
model.add(LSTM(units=units,activation='tanh', input_shape=(step_size,nb_features),return_sequences=False))
model.add(Dropout(0.8))
model.add(Dense(output_size))
model.add(LeakyReLU())
model.compile(loss='mse', optimizer='adam')
model.fit(training_datas, training_labels, batch_size=batch_size,validation_data=(validation_datas,validation_labels), epochs = epochs, callbacks=[CSVLogger(output_file_name+'.csv', append=True),ModelCheckpoint('weights/'+output_file_name+'-{epoch:02d}-{val_loss:.5f}.hdf5', monitor='val_loss', verbose=1,mode='min')])
```

<br></br>

LSTMì€ ì»¤ë„ í¬ê¸°, strides, ì…ë ¥ ì‚¬ì´ì¦ˆ ë° ì¶œë ¥ ì‚¬ì´ì¦ˆ ê°„ì˜ ê´€ê³„ë¥¼ ì‹ ê²½ ì“¸ í•„ìš”ê°€ ì—†ìœ¼ë¯€ë¡œ êµ¬í˜„ì´ CNNë³´ë‹¤ ìƒëŒ€ì ìœ¼ë¡œ ì‰½ìŠµë‹ˆë‹¤. ì…ë ¥ ë° ì¶œë ¥ì˜ ì‚¬ì´ì¦ˆê°€ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì˜¬ë°”ë¥´ê²Œ ì •ì˜ë˜ì—ˆëŠ”ì§€ë§Œ í™•ì¸í•˜ì„¸ìš”!

```Python
model = Sequential()
model.add(LSTM(units=units,activation='tanh', input_shape=(step_size,nb_features),return_sequences=False))
model.add(Dropout(0.8))
model.add(Dense(output_size))
model.add(LeakyReLU())
model.compile(loss='mse', optimizer='adam')
```

<br></br>
<br></br>

#### GRU

Gated Recurrent Unit(Gated Recurrent Unit)ì€ RNNì˜ ë˜ ë‹¤ë¥¸ ë³€í˜•ì…ë‹ˆë‹¤. GRUì˜ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ëŠ” 1ê°œì˜ reset, forget ê²Œì´íŠ¸ê°€ ìˆì–´ì„œ LSTMë³´ë‹¤ ëœ ì •êµí•˜ì§€ë§Œ GRUì˜ ì„±ëŠ¥ì€ LSTMê³¼ ë™ì¼í•©ë‹ˆë‹¤. ë”°ë¼ì„œ LSTM ë³´ë‹¤ íš¨ìœ¨ì„±ì´ ë” ë†’ë‹¤ëŠ” ì£¼ì¥ì´ ìˆìŠµë‹ˆë‹¤. (ì´ íŠœí† ë¦¬ì–¼ì—ì„œë„ LSTMëŠ” ì•½ 45ì´ˆ/epoch, GRUëŠ” 40ì´ˆ/epoch ì±„ ê±¸ë¦¬ì§€ ì•Šê¸° ë•Œë¬¸ì— ì£¼ì¥ì´ ë§ë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)

<br></br>

![GRU](./media/21_2.png)

*figre3 : GRU Illustration (retrieved from http://www.jackdermody.net/brightwire/article/GRU_Recurrent_Neural_Networks)*

<br></br>

```Python
import pandas as pd
import numpy as numpy
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten,Reshape
from keras.layers import Conv1D, MaxPooling1D, LeakyReLU
from keras.utils import np_utils
from keras.layers import GRU,CuDNNGRU
from keras.callbacks import CSVLogger, ModelCheckpoint
import h5py
import os
import tensorflow as tf
from keras.backend.tensorflow_backend import set_session



os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

config = tf.ConfigProto()
config.gpu_options.allow_growth = True
set_session(tf.Session(config=config))

with h5py.File(''.join(['bitcoin2015to2017_close.h5']), 'r') as hf:
    datas = hf['inputs'].value
    labels = hf['outputs'].value


output_file_name='bitcoin2015to2017_close_GRU_1_tanh_relu_'

step_size = datas.shape[1]
units= 50
batch_size = 8
nb_features = datas.shape[2]
epochs = 100
output_size=16
#split training validation
training_size = int(0.8* datas.shape[0])
training_datas = datas[:training_size,:]
training_labels = labels[:training_size,:,0]
validation_datas = datas[training_size:,:]
validation_labels = labels[training_size:,:,0]

#build model
model = Sequential()
model.add(GRU(units=units, input_shape=(step_size,nb_features),return_sequences=False))
model.add(Activation('tanh'))
model.add(Dropout(0.2))
model.add(Dense(output_size))
model.add(Activation('relu'))
model.compile(loss='mse', optimizer='adam')
model.fit(training_datas, training_labels, batch_size=batch_size,validation_data=(validation_datas,validation_labels), epochs = epochs, callbacks=[CSVLogger(output_file_name+'.csv', append=True),ModelCheckpoint('weights/'+output_file_name+'-{epoch:02d}-{val_loss:.5f}.hdf5', monitor='val_loss', verbose=1,mode='min')])
```

<br></br>

ê°„ë‹¨í•˜ê²Œ LSTMì„ ë¹Œë“œí•˜ëŠ” ë‘ ë²ˆì§¸ ì½”ë“œë¼ì¸ì„ GRUë¡œ ë°”ê¾¸ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.

```Python
model.add(LSTM(units=units,activation='tanh', input_shape=(step_size,nb_features),return_sequences=False))
```

ì„

```Python
model.add(GRU(units=units,activation='tanh', input_shape=(step_size,nb_features),return_sequences=False))
```

ë¡œ ë°”ê¾¸ì„¸ìš”!

<br></br>
<br></br>

#### Result Plotting

ê²°ê³¼ê°€ ì„¸ ê°€ì§€ ëª¨ë¸ê³¼ ëª¨ë‘ ë¹„ìŠ·í•˜ê¸° ë•Œë¬¸ì— CNN ë²„ì „ë§Œ ë³´ì—¬ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ ì¬êµ¬ì„±í•˜ê³  í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ ëª¨ë¸ì— ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.

<br></br>

```Python
from keras import applications
from keras.models import Sequential
from keras.models import Model
from keras.layers import Dropout, Flatten, Dense, Activation
from keras.callbacks import CSVLogger
import tensorflow as tf
from scipy.ndimage import imread
import numpy as np
import random
from keras.layers import LSTM
from keras.layers import Conv1D, MaxPooling1D, LeakyReLU
from keras import backend as K
import keras
from keras.callbacks import CSVLogger, ModelCheckpoint
from keras.backend.tensorflow_backend import set_session
from keras import optimizers
import h5py
from sklearn.preprocessing import MinMaxScaler
import os
import pandas as pd
# import matplotlib

import matplotlib.pyplot as plt

os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

with h5py.File(''.join(['bitcoin2015to2017_close.h5']), 'r') as hf:
    datas = hf['inputs'].value
    labels = hf['outputs'].value
    input_times = hf['input_times'].value
    output_times = hf['output_times'].value
    original_inputs = hf['original_inputs'].value
    original_outputs = hf['original_outputs'].value
    original_datas = hf['original_datas'].value


scaler=MinMaxScaler()
#split training validation
training_size = int(0.8* datas.shape[0])
training_datas = datas[:training_size,:,:]
training_labels = labels[:training_size,:,:]
validation_datas = datas[training_size:,:,:]
validation_labels = labels[training_size:,:,:]
validation_original_outputs = original_outputs[training_size:,:,:]
validation_original_inputs = original_inputs[training_size:,:,:]
validation_input_times = input_times[training_size:,:,:]
validation_output_times = output_times[training_size:,:,:]

ground_true = np.append(validation_original_inputs,validation_original_outputs, axis=1)
ground_true_times = np.append(validation_input_times,validation_output_times, axis=1)
step_size = datas.shape[1]
batch_size= 8
nb_features = datas.shape[2]

model = Sequential()

# 2 layers
model.add(Conv1D(activation='relu', input_shape=(step_size, nb_features), strides=3, filters=8, kernel_size=20))
# model.add(LeakyReLU())
model.add(Dropout(0.25))
model.add(Conv1D( strides=4, filters=nb_features, kernel_size=16))
model.load_weights('weights/bitcoin2015to2017_close_CNN_2_relu-44-0.00030.hdf5')
model.compile(loss='mse', optimizer='adam')
```

<br></br>

ë°ì´í„° í”„ë ˆì„ì— ì‹¤ì œ ê°€ê²©(ì‹¤ì œ ê°€ê²©)ê³¼ ë¹„íŠ¸ì½”ì¸ì˜ ì˜ˆìƒ ê°€ê²©ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ì‹œê°í™”ë¥¼ ìœ„í•´ í‘œì‹œëœ ìˆ˜ì¹˜ëŠ” 2017ë…„ 8ì›”ê³¼ ê·¸ ì´í›„ì˜ ë°ì´í„°ë§Œ ë³´ì—¬ì¤ë‹ˆë‹¤.

<br></br>

```pythonn
ground_true_df = pd.DataFrame()
ground_true_df['times'] = ground_true_times
ground_true_df['value'] = ground_true

prediction_df = pd.DataFrame()
prediction_df['times'] = validation_output_times
prediction_df['value'] = predicted_inverted

prediction_df = prediction_df.loc[(prediction_df["times"].dt.year == 2017 )&(prediction_df["times"].dt.month > 7 ),: ]
ground_true_df = ground_true_df.loc[(ground_true_df["times"].dt.year == 2017 )&(ground_true_df["times"].dt.month > 7 ),:]
```
<br></br>

pyplotì„ ì´ìš©í•´ì„œ í”Œë¡¯ì„ ê·¸ë¦½ë‹ˆë‹¤. ì˜ˆìƒ ê°€ê²©ì€ 16ë¶„ ê¸°ì¤€ì´ê¸° ë•Œë¬¸ì—, ê·¸ê²ƒë“¤ ëª¨ë‘ë¥¼ ì—°ê²°í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ê²°ê³¼ê°€ ë” ì‰½ê²Œ ì´í•´ë  ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì—¬ê¸°ì„œ ì˜ˆì¸¡ ë°ì´í„°ëŠ” ì„¸ ë²ˆì§¸ í–‰ì˜ "ro"ê°€ ë‚˜íƒ€ë‚´ëŠ” ë¹¨ê°„ìƒ‰ ì ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤. ì•„ë˜ figure4ì— ë³´ì´ëŠ” ê·¸ë˜í”„ì˜ íŒŒë€ìƒ‰ ì„ ì€ ì‹¤ì œ ë°ì´í„°ë¥¼ ë‚˜íƒ€ë‚´ë©° ë¹¨ê°„ìƒ‰ ì ì€ ì˜ˆìƒë˜ëŠ” ë¹„íŠ¸ì½”ì¸ ê°€ê²©ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

```pythonn
plt.figure(figsize=(20,10))
plt.plot(ground_true_df.times,ground_true_df.value, label = 'Actual')
plt.plot(prediction_df.times,prediction_df.value,'ro', label='Predicted')
plt.legend(loc='upper left')
plt.show()
```

<br></br>

![result](./media/21_3.png)

*figure4: Best Result Plot for Bitcoin Price Prediction With 2-Layered CNN*

<br></br>

figure4ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ì˜ˆì¸¡ëœ ê°’ì€ ë¹„íŠ¸ì½”ì¸ì˜ ì‹¤ì œ ê°€ê²©ê³¼ ë§¤ìš° ìœ ì‚¬í•©ë‹ˆë‹¤. ê°€ì¥ ì¢‹ì€ ëª¨ë¸ì„ ì„ íƒí•˜ê¸° ìœ„í•´, ì €ëŠ” ì•„ë˜ figure5 í‘œë¥¼ ë§Œë“¤ì–´ì„œ ë„¤íŠ¸ì›Œí¬ì˜ ëª‡ ê°€ì§€ êµ¬ì„±ì„ í…ŒìŠ¤íŠ¸í•˜ê¸°ë¡œ ê²°ì •í–ˆìŠµë‹ˆë‹¤.

<br></br>

![table](./media/21_4.png)

*figure5 : Prediction Results for Different Models*

<br></br>

figure5ì— ë³´ì´ëŠ” í‘œëŠ” 100ë²ˆì˜ í•™ìŠµ epochsì—ì„œ ê°€ì¥ ìš°ìˆ˜í•œ validation loss(ì†ì‹¤)ë¥¼ ë„ì¶œí•˜ëŠ” ëª¨ë¸ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìš°ë¦¬ëŠ” LeakyReLUê°€ ReLUë³´ë‹¤ ë” ì ì€ ì†ì‹¤ì„ ë°œìƒì‹œí‚¤ëŠ” ê²ƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ í™œì„±í•¨ìˆ˜(activation function)ë¡œ LeakyReLUë¥¼ ì‚¬ìš©í•œ 4-layer(ë ˆì´ì–´) CNNì´ í° validation lossë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤. ì´ê²ƒì€ ë‹¤ì‹œ validationì´ í•„ìš”í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì˜ ì˜ëª»ëœ ë°°ì¹˜ ë•Œë¬¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ í‘œë¥¼ ë³´ê³  CNN ëª¨ë¸ì€ LSTM, GRU ë³´ë‹¤ í¼í¬ë¨¼ìŠ¤ëŠ” ì¡°ê¸ˆ ë‚˜ì˜ì§€ë§Œ êµ‰ì¥íˆ ë¹ ë¥´ê²Œ í•™ìŠµ (2ì´ˆ/ epoch, GPUì‚¬ìš©)ë  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 3-layer CNNì´ ë°ì´í„°ì˜ ì§€ì—­ì  íŠ¹ì§•ì„ ì˜ ì¡ì•„ë‚´ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì´ì§€ë§Œ, ê°€ì¥ ì¢‹ì€ ëª¨ë¸ì€ LSTMì„ tanhì™€ LeakyReLUë¥¼ ì‚¬ìš©í•œ ê²ƒì…ë‹ˆë‹¤.

<br></br>

![LSTM with tanh and Leaky ReLu](./media/21_5.png)

*figure6 : LSTM with tanh and Leaky ReLu as activation function*

<br></br>

![LSTM with tanh and Leaky ReLu](./media/21_6.png)

*figure7 : 3-layered CNN with Leaky ReLu as activation function.*

<br></br>

ë¹„ë¡ ì˜ˆì¸¡ëœ ê²°ê³¼ê°€ ì¢‹ì•„ë³´ì—¬ë„, overfittinng(ê³¼ì í•©)ì˜ ê±±ì •ì´ ìˆìŠµë‹ˆë‹¤. LSTMì„ LeakyReLUë¡œ í•™ìŠµí•  ë•Œì˜ loss(ì†ì‹¤)ê³¼ validation(ê²€ì¦)ì˜ lossì—ëŠ” ê°­ì´ ìˆìŠµë‹ˆë‹¤(5.97E-06 vs 3.92E-05). ì´ ì†ì‹¤ì„ ìµœì†Œí™” í•˜ê¸° ìœ„í•´ì„œ Regularization(ì •ê·œí™”)ê°€ í•„ìš”í•©ë‹ˆë‹¤.

<br></br>

#### Regularization

ê°€ì¥ ì¢‹ì€ regularization ì „ëµì„ ì°¾ê¸° ìœ„í•´ì„œ, L1 regularization, L2 regularizationìœ¼ë¡œ ëª‡ ê°œì˜ ì‹¤í—˜ì„ í–ˆìŠµë‹ˆë‹¤. ì²« ë²ˆì§¸ë¡œ ìš°ë¦¬ëŠ” LSTMì— ë°ì´í„°ë¥¼ ì‰½ê²Œ í”¼íŒ…ì‹œí‚¬ ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤. ì €ëŠ” biasë¥¼ ë²¡í„°ì— ëŒ€í•œ regularizationì¸ bias regularizerë¥¼ ì˜ˆë¡œ ë“¤ê² ìŠµë‹ˆë‹¤.

<br></br>

```python
def fit_lstm(reg):
    global training_datas, training_labels, batch_size, epochs,step_size,nb_features, units
    model = Sequential()
    model.add(CuDNNLSTM(units=units, bias_regularizer=reg, input_shape=(step_size,nb_features),return_sequences=False))
    model.add(Activation('tanh'))
    model.add(Dropout(0.2))
    model.add(Dense(output_size))
    model.add(LeakyReLU())
    model.compile(loss='mse', optimizer='adam')
    model.fit(training_datas, training_labels, batch_size=batch_size, epochs = epochs, verbose=0)
    return model
```
<br></br>

ì‹¤í—˜ì€ ëª¨ë¸ì„ 30ë²ˆ ê°ê° 30 epochsì”© ì§„í–‰í•©ë‹ˆë‹¤.

<br></br>

```python
def experiment(validation_datas,validation_labels,original_datas,ground_true,ground_true_times,validation_original_outputs, validation_output_times, nb_repeat, reg):
    error_scores = list()
    #get only the close data
    ground_true = ground_true[:,:,0].reshape(-1)
    ground_true_times = ground_true_times.reshape(-1)
    ground_true_times = pd.to_datetime(ground_true_times, unit='s')
    validation_output_times = pd.to_datetime(validation_output_times.reshape(-1), unit='s')
    for i in range(nb_repeat):
        model = fit_lstm(reg)
        predicted = model.predict(validation_datas)
        predicted_inverted = []
        scaler.fit(original_datas[:,0].reshape(-1,1))
        predicted_inverted.append(scaler.inverse_transform(predicted))
        # since we are appending in the first dimension
        predicted_inverted = np.array(predicted_inverted)[0,:,:].reshape(-1)
        error_scores.append(mean_squared_error(validation_original_outputs[:,:,0].reshape(-1),predicted_inverted))
    return error_scores

regs = [regularizers.l1(0),regularizers.l1(0.1), regularizers.l1(0.01), regularizers.l1(0.001), regularizers.l1(0.0001),regularizers.l2(0.1), regularizers.l2(0.01), regularizers.l2(0.001), regularizers.l2(0.0001)]
nb_repeat = 30
results = pd.DataFrame()
for reg in regs:

    name = ('l1 %.4f,l2 %.4f' % (reg.l1, reg.l2))
    print "Training "+ str(name)
    results[name] = experiment(validation_datas,validation_labels,original_datas,ground_true,ground_true_times,validation_original_outputs, validation_output_times, nb_repeat,reg)

results.describe().to_csv('result/lstm_bias_reg.csv')
results.describe()
view rawdef experiment(validation_datas,validation_labels,original_datas,ground_true,ground_true_times,validation_original_outputs, validation_output_times, nb_repeat, reg):
    error_scores = list()
    #get only the close data
    ground_true = ground_true[:,:,0].reshape(-1)
    ground_true_times = ground_true_times.reshape(-1)
    ground_true_times = pd.to_datetime(ground_true_times, unit='s')
    validation_output_times = pd.to_datetime(validation_output_times.reshape(-1), unit='s')
    for i in range(nb_repeat):
        model = fit_lstm(reg)
        predicted = model.predict(validation_datas)
        predicted_inverted = []
        scaler.fit(original_datas[:,0].reshape(-1,1))
        predicted_inverted.append(scaler.inverse_transform(predicted))
        # since we are appending in the first dimension
        predicted_inverted = np.array(predicted_inverted)[0,:,:].reshape(-1)
        error_scores.append(mean_squared_error(validation_original_outputs[:,:,0].reshape(-1),predicted_inverted))
    return error_scores

regs = [regularizers.l1(0),regularizers.l1(0.1), regularizers.l1(0.01), regularizers.l1(0.001), regularizers.l1(0.0001),regularizers.l2(0.1), regularizers.l2(0.01), regularizers.l2(0.001), regularizers.l2(0.0001)]
nb_repeat = 30
results = pd.DataFrame()
for reg in regs:

    name = ('l1 %.4f,l2 %.4f' % (reg.l1, reg.l2))
    print "Training "+ str(name)
    results[name] = experiment(validation_datas,validation_labels,original_datas,ground_true,ground_true_times,validation_original_outputs, validation_output_times, nb_repeat,reg)

results.describe().to_csv('result/lstm_bias_reg.csv')
results.describe()
view raw
```

<br></br>

ë§Œì•½ ë‹¹ì‹ ì´ Jupyter notebookì„ ì‚¬ìš©í•˜ê³  ìˆë‹¤ë©´, ì•„ë˜ì˜ figure8ì— ë³´ì´ëŠ” í‘œë¥¼ ê²°ê³¼ë¡œë¶€í„° ë°”ë¡œ í™•ì¸í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br></br>

![Result of Running Bias Regularizer](./media/21_7.png)

*figure8 : Result of Running Bias Regularizer*

<br></br>

```python
results.describe().boxplot()
plt.show()
```

<br></br>

ë¹„êµë¥¼ ìœ„í•´ ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ê¸° ìœ„í•´, boxplotì„ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤:

![Result of Running Bias Regularizer](./media/21_8.png)

*figure9*

<br></br>

ë¹„êµì— ë”°ë¥´ë©´ bias ë²¡í„°ì— ëŒ€í•œ ê³„ìˆ˜ 0.01ì˜ L2 ì •ê·œí™”ê¸°ê°€ ìµœì„ ì˜ ê²°ê³¼ë¥¼ ë„ì¶œí•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.
activation, bias, ì»¤ë„, recurrent ë§¤íŠ¸ë¦­ìŠ¤ ë“±ì˜ ëª¨ë“  regularizer ì¤‘ì—ì„œ ìµœìƒì˜ ì¡°í•©ì„ í™•ì¸í•˜ë ¤ë©´ ëª¨ë“  ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ í•˜ë‚˜ì”© í…ŒìŠ¤íŠ¸í•´ì•¼ í•˜ëŠ”ë°, ì´ëŠ” í˜„ì¬ í•˜ë“œì›¨ì–´ êµ¬ì„±ìœ¼ë¡œëŠ” ì–´ë ¤ì›Œ ë³´ì…ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, ì €ëŠ” ê·¸ê²ƒì„ ë¯¸ë˜ ê³„íšìœ¼ë¡œ ë‚¨ê¸¸ ê²ƒì…ë‹ˆë‹¤.

<br></br>
<br></br>

#### Conclusion

ë‹¹ì‹ ì€ ì•„ë˜ì˜ íŠœí† ë¦¬ì–¼ì„ ì§„í–‰í•˜ë©´ì„œ ì•„ë˜ì˜ 5ê°€ì§€ë¥¼ ë°°ì› ìŠµë‹ˆë‹¤.

1. ì‹¤ì‹œê°„ ë¹„íŠ¸ì½”ì¸ ë°ì´í„° ìˆ˜ì§‘ ë°©ë²•.
2. í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„ ë°©ë²•.
3. ë”¥ëŸ¬ë‹ì„ ì‚¬ìš©í•˜ì—¬ ë¹„íŠ¸ì½”ì¸ì˜ ê°€ê²©ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•.
4. ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ëŠ” ë°©ë²•.
5. ëª¨ë¸ì— ì •ê·œí™”ë¥¼ ì ìš©í•˜ëŠ” ë°©ë²•.

ì´ íŠœí† ë¦¬ì–¼ì˜ í–¥í›„ ì‘ì—…ì€ ìµœê³ ì˜ ëª¨ë¸ì— ë§ëŠ” ìµœê³ ì˜ hyperparameter(í•˜ì´í¼íŒŒë¼ë¯¸í„°)ë¥¼ ì°¾ê³ , ì†Œì…œ ë¯¸ë””ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ì„¸ë¥¼ ë³´ë‹¤ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì œê°€ ë¯¸ë””ì—„ìœ¼ë¡œ ê²Œì‹œí•˜ëŠ” ê²ƒì€ ì´ë²ˆì´ ì²˜ìŒì…ë‹ˆë‹¤. ì‹¤ìˆ˜ë‚˜ ì§ˆë¬¸ì´ ìˆì„ ê²½ìš° ì£¼ì €í•˜ì§€ ë§ê³  ì•„ë˜ì— ì½”ë©˜íŠ¸ë¥¼ ë‚¨ê²¨ ì£¼ì‹­ì‹œì˜¤.

ë” ë§ì€ ì •ë³´ë¥¼ ì›í•˜ì‹œë©´, ì œ [Github](https://github.com/khuangaf/CryptocurrencyPrediction)ì„ ì°¸ì¡°í•˜ì‹œê³ , ë” ìœ„ëŒ€í•œ ë”¥ëŸ¬ë‹ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë“¤ì„ ìœ„í•´ twitterì—ì„œ ì €ë¥¼ follow í•˜ëŠ” ê²ƒì„ ìŠì§€ë§ˆì„¸ìš”!

<br></br>
<br></br>

#### ì°¸ê³ ìë£Œ
[RNNê³¼ LSTMì„ ì´í•´í•´ë³´ì!](https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/)
[ì›ë¬¸ github ì½”ë“œ](https://github.com/khuangaf/CryptocurrencyPrediction)

<br></br>

> ì´ ê¸€ì€ 2018 ì»¨íŠ¸ë¦¬ë·°í†¤ì—ì„œ Contribute to Keras í”„ë¡œì íŠ¸ë¡œ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.
>
> Translator: ë°•ì •í˜„
>
> Translator email : parkjh688@gmail.com
